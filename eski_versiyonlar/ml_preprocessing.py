import pandas as pd
import re
import string
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

# --- AYARLAR ---
EXCEL_DOSYA_ADI = "sosyal_medya_full_data.xlsx" # Elindeki ana veri dosyasÄ±
KAYIT_DOSYASI_PKL = "tfidf_vectorizer.pkl"     # EÄŸitilmiÅŸ vektÃ¶rleÅŸtiriciyi saklayacaÄŸÄ±z
TEMIZ_VERI_CSV = "ml_hazir_veri.csv"           # TemizlenmiÅŸ veriyi buraya kaydedeceÄŸiz

# --- TÃœRKÃ‡E STOP WORDS ---
STOP_WORDS = {
    "ve", "ile", "bir", "bu", "ÅŸu", "o", "iÃ§in", "da", "de", "ki", "mi", 
    "mu", "ama", "fakat", "lakin", "ancak", "yine", "bÃ¶yle", "ÅŸÃ¶yle", 
    "diye", "bana", "sana", "ben", "sen", "biz", "siz", "onlar", "var", 
    "yok", "Ã§ok", "daha", "kadar", "gibi", "en", "mÄ±", "mu", "mÃ¼", "ÅŸey"
}

# --- BASÄ°T KÃ–K BULUCU (Stemmer) ---
# TÃ¼rkÃ§e NLP kÃ¼tÃ¼phaneleri (Zemberek) kurulumu zor olduÄŸu iÃ§in 
# proje kapsamÄ±nda basit bir kural tabanlÄ± kÃ¶k bulucu kullanÄ±yoruz.
def basit_kok_bul(kelime):
    ekler = ["lar", "ler", "nÄ±n", "nin", "dan", "den", "mÄ±", "mi", "un", "Ã¼n", "im", "sin", "siniz"]
    for ek in ekler:
        if kelime.endswith(ek) and len(kelime) > len(ek) + 2: # KÃ¶ke zarar vermemek iÃ§in kontrol
            return kelime[:-len(ek)]
    return kelime

def metin_temizle(metin):
    """
    Ham metni alÄ±r, ML iÃ§in tertemiz hale getirir.
    """
    try:
        metin = str(metin).lower() # KÃ¼Ã§Ã¼k harf
        
        # 1. Regex TemizliÄŸi
        metin = re.sub(r'http\S+', '', metin) # Linkleri sil
        metin = re.sub(r'<.*?>', '', metin)   # HTML etiketlerini sil
        metin = re.sub(r'\d+', '', metin)     # SayÄ±larÄ± sil
        metin = re.sub(r'[^\w\s]', '', metin) # Noktalama iÅŸaretlerini sil (Emoji dahil)
        
        # 2. Kelime Ä°ÅŸlemleri
        kelimeler = metin.split()
        
        # Stop Words TemizliÄŸi ve KÃ¶k Bulma
        temiz_kelimeler = [
            basit_kok_bul(kelime) for kelime in kelimeler 
            if kelime not in STOP_WORDS and len(kelime) > 2
        ]
        
        return " ".join(temiz_kelimeler)
        
    except:
        return ""

def veri_hazirla():
    print("ğŸ”„ Veri Ã–n Ä°ÅŸleme (Preprocessing) BaÅŸlÄ±yor...")
    
    # 1. Veriyi Oku
    df = pd.read_excel(EXCEL_DOSYA_ADI)
    print(f"âœ… {len(df)} satÄ±r ham veri okundu.")
    
    # 2. Temizlik Yap
    print("ğŸ§¼ Metinler temizleniyor (Regex + Stemming)...")
    df['Temiz_Yorum'] = df['Yorum'].apply(metin_temizle)
    
    # BoÅŸ kalan satÄ±rlarÄ± at (Temizlik sonrasÄ± boÅŸalanlar)
    df = df[df['Temiz_Yorum'].str.len() > 2]
    print(f"âœ… Temizlik sonrasÄ± kalan veri: {len(df)} satÄ±r.")
    
    # 3. Etiketleme (Labeling) - ÅÄ°MDÄ°LÄ°K GEÃ‡Ä°CÄ°
    # ML modelini eÄŸitmek iÃ§in elimizde "DoÄŸru Cevaplar" (Etiketler) olmasÄ± lazÄ±m.
    # Åimdilik kendi yazdÄ±ÄŸÄ±mÄ±z sÃ¶zlÃ¼k tabanlÄ± analiz sonucunu "DoÄŸru Cevap" kabul edeceÄŸiz.
    # (Ä°deal dÃ¼nyada elle etiketlemek gerekirdi ama zamanÄ±mÄ±z yok).
    
    from proje_v2 import TezAnalizSistemi # Ã–nceki kodumuzdan sÄ±nÄ±fÄ± Ã§aÄŸÄ±rÄ±yoruz
    analiz_sistemi = TezAnalizSistemi(EXCEL_DOSYA_ADI)
    df[['Skor', 'Etiket']] = df['Yorum'].apply(analiz_sistemi.sentiment_hesapla)
    
    # Sadece Olumlu/Olumsuz olanlarÄ± al (NÃ¶tr'Ã¼ at, ML kafasÄ± karÄ±ÅŸmasÄ±n)
    df_egitim = df[df['Etiket'] != 'NÃ¶tr'].copy()
    
    # Etiketleri SayÄ±ya Ã‡evir (Olumlu=1, Olumsuz=0)
    df_egitim['Hedef'] = df_egitim['Etiket'].apply(lambda x: 1 if x == 'Olumlu' else 0)
    
    print(f"ğŸ¤– EÄŸitim iÃ§in kullanÄ±lacak veri sayÄ±sÄ±: {len(df_egitim)} (NÃ¶trler Ã§Ä±karÄ±ldÄ±)")
    
    # 4. CSV Olarak Kaydet (Model bu dosyayÄ± kullanacak)
    df_egitim.to_csv(TEMIZ_VERI_CSV, index=False)
    print(f"ğŸ’¾ HazÄ±r veri kaydedildi: {TEMIZ_VERI_CSV}")

    return df_egitim

if __name__ == "__main__":
    veri_hazirla()